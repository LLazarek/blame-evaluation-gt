The reader may have observed that mutators that produce 
type-level mistakes do not necessarily turn our benchmarks into
interesting debugging scenarios for the rational programmer.  The
benchmarks are fully typed, and type checking detects each mistake at
compile time.  Hence, we apply the mutators to versions, dubbed
dubbed{scenarios}, of the benchmarks without 
some  of their type annotations.  Without the annotations, the type
checker may not detect a mistake at compile time and, instead, the run
time checks
of the gradual type system signal an error.  Scenarios
with type-level mistakes that result in run time errors are the ideal
ground to exercise the rational programmer's ability to locate errors.

\begin{figure*}
  \begin{tabular}{p{2.1cm} | p{7cm}  | p{7.5cm} }
    {\bf name} & {\bf description} & {\bf example} \\
\hline

    \texttt{constant}
& Swaps a constant with another of the same value but different type.
& \texttt{5.6} $\rightarrow$ \texttt{5.6+0.0i} \\
\hline

    \texttt{deletion}
    & Deletes the result expression of a sequence.
    & \texttt{(begin x y z)} $\rightarrow$ \texttt{(begin x y)} \\
\hline

   \texttt{position}
 & Swaps two subexpressions.
  &  \texttt{(f a 42 "b" 0)} $\rightarrow$\texttt{(f a 42 0 "b")} \\
\hline

     \texttt{list}
    & Replaces \texttt{append} with \texttt{cons}.
    &  \begin{tabular}[t]{@{}l}
      \texttt{append} $\rightarrow$ \texttt{cons}
    \end{tabular}\\
\hline

    \texttt{top-level-id}
 & Swaps identifiers defined in the same module.
 &\texttt{(f x 42)} $\rightarrow$ \texttt{(g x 42)}\\
\hline

    \texttt{imported-id}
 & Swaps identifiers imported from the same module.
 &\texttt{(f x 42)} $\rightarrow$ \texttt{(g x 42)}\\
\hline

    \texttt{method-id}
 & Swaps two method identifiers.
 &\texttt{(send o f x 42)} $\rightarrow$ \texttt{(send o g x 42)}\\
\hline

    \texttt{field-id}
 & Swaps two field identifiers.
    &  \texttt{(get-field o f)} $\rightarrow$  \texttt{(get-field o g)}\\
\hline

     \texttt{class:init}
  & Swaps values of class initializers.
   &  \begin{tabular}[t]{@{}l}
     \texttt{(new c [a 5] [b "hello"])} $\rightarrow$\\
     \texttt{(new c [a "hello"] [b 5])}\\
    \end{tabular}\\
\hline

     \texttt{class:parent}
    & Replaces the parent of classes with \texttt{object\%}.
    &  \begin{tabular}[t]{@{}l}
     \texttt{(class foo\% (super-new))} $\rightarrow$\\
     \texttt{(class object\% (super-new))}\\
    \end{tabular}\\
\hline

     \texttt{class:public}
    & Makes a public method private and vice versa.
    & \begin{tabular}[t]{@{}l}
      \texttt{(class object (define/public (m x) x))} $\rightarrow$\\
      \texttt{(class object (define/private (m x) x))}\\
    \end{tabular}\\
\hline

     \texttt{class:super}
    & Removes \texttt{super-new} calls from class definitions.
    & \begin{tabular}[t]{@{}l}
      \texttt{(class foo\% (super-new))} $\rightarrow$ \\
      \texttt{(class foo\% (void))}\\
       \end{tabular}\\
\hline


     \texttt{arithmetic}
& Swaps arithmetic operators.
    & \texttt{+} $\rightarrow$ \texttt{-} \\
\hline

     \texttt{boolean}
& Swaps \texttt{and} and \texttt{or}.
& \texttt{and} $\rightarrow$ \texttt{or} \\
\hline

     \texttt{negate-cond}
    & Negates conditional test expressions.
    &  \texttt{(if (= x 0) t e)} $\rightarrow$ \texttt{(if (not (= x 0)) t e)}\\
\hline

     \texttt{force-cond}
    & Replaces conditional test expressions with \texttt{\#t}.
    & \texttt{(if (= x 0) t e)} $\rightarrow$ \texttt{(if \#t t e)}
\end{tabular}
  \caption{Summary of mutators.}
  \label{table:mutation-ops}
\end{figure*}

Specifically, we define an \emph{interesting debugging scenario} as a
scenario that
\begin{enumerate} 
  \item is rejected by the type checker when we restore all its missing type annotations, 
  \item produces a run time error under Erasure as-is, and 
  \item the stack trace of the error mentions at least three distinct components.  
\end{enumerate}

The first criterion simply validates that the debugging scenario concerns
an actual type-level mistake. The second one stipulates that the mistake is
detectable under Erasure to ensure that it is detectable by all three
semantics.   Whether a scenario produces a run time error largely depends on 
the underlying semantics but \citet{gfd-oopsla-2019}
show that a run time error under Erasure implies a run time
error under both Transient and Natural. Thus the criterion aligns
interesting debugging scenarios with our lowest denominator, Erasure, 
and excludes  from consideration the very interesting scenarios indeed where Natural or
Transient can detect a mistake but where Erasure is powerless. Of course this choice favors
Erasure over Transient and Natural and, for the same reason, Transient over
Natural. We consider some form of bias towards one or the other semantics
unavoidable no matter how we define interesting debugging scenarios and we
opt for tipping the scales in favor of the theoretically weakest
semantics. We further discuss the implications of this choice in
section~\ref{sec:discussion}.

The third criterion aims to capture debugging scenarios with a specific
kind of run time error: one produced because of the interaction between the buggy
portion of the program with the rest of the program.  In these cases, the rational programmer
may need to examine several pieces of the program to locate the source of the faulty interaction.
In technical terms, we reliably mark these scenarios by
analyzing the stack trace of a run time error.  If the stack trace mentions at
least two distinct components, then this indicates interaction.  We require a third component because our benchmarks come
with a driver component which is present in the stack trace of all run
time errors. In essence, this last criterion excludes a large number of
trivial to debug, and thus ineffective, scenarios that error immediately when their mutated component is
evaluated. 


%\begin{figure*}
%  \centering
%  \includegraphics[scale=0.35]{./plots/mutant-breakdown}
%  \caption{Interesting debugging scenarios produced by our mutators. Counts are cut off at 50.}
%  \label{fig:mutant-breakdown}
%\end{figure*}


With the definition of interesting debugging scenarios in hand, we analyze how effective our mutators
are for turning the benchmarks into interesting debugging scenarios.
At a high level, the mutators produce 16,800 mutants with at least one interesting debugging scenario each across all of our benchmarks.
Broken down by benchmark, the mutators produce at least 40 interesting
scenarios for every benchmark, and these scenarios originate from at
least four different mutators per benchmark.  Thus the mutators result in 
a sizeable and diverse population of scenarios for every benchmark.
Furthermore, every mutator contributes scenarios to at least one
benchmark.
That being said, some mutators only apply to a small subset of the benchmarks;
those mutators target specific features which only those benchmarks make us of, so we include them despite their narrow applicability.
For instance, the class-focused mutators are mainly effective in \texttt{take5}, since it is the benchmark with the most extensive use of object-oriented features.
