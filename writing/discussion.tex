%% -----------------------------------------------------------------------------
\section{What We Learned}

\begin{itemize}
  \item Interesting discoveries...?

  \item Comparison of existing GT systems represented in our study.  
    Natural is here, transient is here, erasure is here, and they
        compare as...

      \item Threats to validity:
        \begin{itemize}
          \item Racket stack traces
          \item Mutant sampling
          \item Blame trails do not necessarily capture all aspects of error reporting
          \item Transient blame adaptations
          \item More?

        \end{itemize}
\end{itemize}        

Like {\em homo economicus\/}, which decouples the actual behavior of a
participant in an economy for the sake of mathematical modeling, the model of a
rational programmer decouples the actual debugging behavior of a software
developer for the sake of a systematic, large-scale analysis. This decoupling
comes advantages and disadvantages. In the economic realm, mathematical models
have provided some predictive insights into the market's behavior; but as
behavioral economics has shown more recently, the mathematical abstraction of a
rational actor makes predictions also quite unreliable in some situations.
\footnote{It has also misled economists to focus on just the mathematics, though
this problem is not relevant here.}  Just like an ordinary consumer or producer,
an actual software developer is unlikely to stick to the exact strategy proposed
here. When this happens, the predicted benefits of blame assignment may not
materialize. Indeed, our own personal experience suggests such deviations, and
it also suggests that deviating is a mistake. To make a true judgment of the
usefulness of the rational-programmer idea, the community will need a lot more
experience with this form of evaluation and relating the evaluation to the
behavior of working programmers.

Our setup hides the type choice a rational programmer must make. When the
run-time checks signal an impedance mismatch in the real world, a programmer
does not have a typed component ready to swap in. Instead, the programmer must
come up with the next set of types---and this means the programmer must make
choices. It is normally possible to assign consistent types to variables in a
component in different ways. The creation and curation of the benchmarks over
many years has driven home this lesson, but fortunately, it has also shown that
the types are in most cases reasonably canonical.  We therefore conjecture that
a rational programmer would in most cases come up with an equivalent type
assignment for trail extension as our experimental setup describes.


\subsection{Threat: Is our Transient Correct?}
%% purpose: talk about transient-blame implementation .... what did we do,
%%  how did we test, what threats remain?
%%
%% the benchmark timeouts look like a threat, but this section shows that
%%  we've looked carefully at those


An apparent flaw in our transient is the high cost of blame.
Whereas \citet{vss-popl-2017} report an average slowdown of 2.5x and
 worst-case of 5.4x on fully-typed benchmarks in Reticulated,
 our configurations frequently exceed a 10 minute timeout with transient blame.
Unfortunately, our high cost is closer to the truth;
 there are at least three broad issues that skew the earlier results.
First, the 2017 implementation of Reticulated fails to insert certain
 soundness checks\footnote{Missing check: \url{https://github.com/mvitousek/reticulated/issues/36}}
 and blame-map updates\footnote{Missing cast: \url{https://github.com/mvitousek/reticulated/issues/43}}
 from the paper.
Second, Reticulated's type checker assigns the dynamic type to local
 variables in the "fully typed" benchmarks---often because the type system
 cannot articulate a more precise type.
Code that ends up with the dynamic type has fewer constraints to check at run-time.
Third, \citet{vss-popl-2017} use relatively small benchmarks.
Four have since been retired from the official Python benchmark suite
 because they are too small, unrealistic, and unstable.\footnote{Release notes: \url{https://pyperformance.readthedocs.io/changelog.html}}
On larger programs, Reticulated suffers from high overhead.
For example, a Reticulated version of our \texttt{sieve} benchmark runs in
 ~40 seconds without blame, and with blame times out after 10 minutes.

%% NOTE: Vitousek's benchmarks are from the Python "pyperformance" suite.
%%   The version notes in the docs talk about retiring benchmarks, but
%%   you can also look at the current codebase and see what names from POPL'17
%%   are missing: callsimple, callmethod, callmethodslots, & pystone
%% <https://github.com/python/pyperformance/tree/master/pyperformance/benchmarks>



