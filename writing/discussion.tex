%% -----------------------------------------------------------------------------
\section{What We Learned}

\begin{itemize}
\item Why does the data look like this?
  - why tr fails
    - bad runtime errors
    - TR bug
  - why transient fails
    - bad runtime errors
    - timeouts/ooms: give context wrt fully untyped/typed versions
    - empty blame: trim Ben's prose

  \item Any kind of dynamic checking has a large positive impact, whether blame or stacktraces are used.
    \begin{itemize}
    \item Both in terms of usefulness and programmer effort
    \item Erasure extremely often fails to provide any helpful information.
    \item That said, there are no long trails for Erasure.
    \end{itemize}

  \item Blame helps significantly over just using stacktraces for both Natural and Transient.
    \begin{itemize}
    \item Natural's blame improves over exceptions by nearly 20\%
    \item Transient's blame improves over exceptions half as much
    \end{itemize}

  \item Natural clearly offers the most utility, but Transient is not too far behind.
    \begin{itemize}
    \item Natural clearly improves over all other modes
    \item Transient improves significantly over all other modes except Natural.
    \item Keep in mind that our selection of scenarios is biased toward Erasure & Transient.
    \end{itemize}
    

  \item Discussion of how confidence / margin of error should be interpreted wrt these results?


   \item Threats to validity:
        \begin{itemize}
          \item Racket stack traces
          \item Mutant sampling
          \item Blame trails do not necessarily capture all aspects of error reporting
          \item Transient blame adaptations
          \item More?

   \end{itemize}
\end{itemize}        

Like {\em homo economicus\/}, which decouples the actual behavior of a
participant in an economy for the sake of mathematical modeling, the model of a
rational programmer decouples the actual debugging behavior of a software
developer for the sake of a systematic, large-scale analysis. This decoupling
comes advantages and disadvantages. In the economic realm, mathematical models
have provided some predictive insights into the market's behavior; but as
behavioral economics has shown more recently, the mathematical abstraction of a
rational actor makes predictions also quite unreliable in some situations.
\footnote{It has also misled economists to focus on just the mathematics, though
this problem is not relevant here.}  Just like an ordinary consumer or producer,
an actual software developer is unlikely to stick to the exact strategy proposed
here. When this happens, the predicted benefits of blame assignment may not
materialize. Indeed, our own personal experience suggests such deviations, and
it also suggests that deviating is a mistake. To make a true judgment of the
usefulness of the rational-programmer idea, the community will need a lot more
experience with this form of evaluation and relating the evaluation to the
behavior of working programmers.

Our setup hides the type choice a rational programmer must make. When the
run-time checks signal an impedance mismatch in the real world, a programmer
does not have a typed component ready to swap in. Instead, the programmer must
come up with the next set of types---and this means the programmer must make
choices. It is normally possible to assign consistent types to variables in a
component in different ways. The creation and curation of the benchmarks over
many years has driven home this lesson, but fortunately, it has also shown that
the types are in most cases reasonably canonical.  We therefore conjecture that
a rational programmer would in most cases come up with an equivalent type
assignment for trail extension as our experimental setup describes.


\subsection{Threat: Is our Transient Correct?}
%% purpose: talk about transient-blame implementation .... what did we do,
%%  how did we test, what threats remain?
%%
%% the benchmark timeouts look like a threat, but this section shows that
%%  we've looked carefully at those

Surprisingly, Transient blames an empty set of boundaries in XXXX cases.
Such cases should never occur, at least in theory, because an empty set
 means the value has never crossed a boundary---if the value is indeed defined
 in the current module, then we have typed code blaming itself for a typed
 value; thus, the type checker must be unsound.
After careful investigation of these empty blame cases, we found no soundness
 bugs in Typed Racket.
Instead, we found scenarios in which Transient lost track of the proper
 boundaries.
These scenarios suggest ways to improve the blame algorithm from~\citet{vss-popl-2017}:
\begin{itemize}
  \item
    Entries in a blame map must point to several parent entries.
    For example, if the function \texttt{f} receives bad input in the call
    \texttt{(filter f xs)}, then blame should point to both \texttt{filter}
    and the \texttt{xs} list.
  \item
    The construction of blame map entries must be guided by type-like specifications
     instead of relying on syntax.
    Aliasing the built-in \texttt{filter} function should not change the shape
     of the blame map.
  \item
    The initial blame map must reflect the initial type environment.
    Typed Racket trusts that untyped core-library functions, such as \texttt{filter},
     behave correctly, but does not list the assumption in the blame map.
\end{itemize}
\noindent{}Despite these known limitations, we conjecture that these improvements
 to Transient blame-tracking will not affect our overall conclusions.

A second apparent flaw in our Transient semantics is the high cost of blame.
Whereas \citet{vss-popl-2017} report an average slowdown of 2.5x and
 worst-case of 5.4x on fully-typed benchmarks in Reticulated,
 our configurations frequently exceed a 10 minute timeout with transient blame.
Unfortunately, our high cost is closer to the truth;
 there are at least three broad issues that skew the earlier results.
First, the 2017 implementation of Reticulated fails to insert certain
 soundness checks\footnote{Missing check: \url{https://github.com/mvitousek/reticulated/issues/36}}
 and blame-map updates\footnote{Missing cast: \url{https://github.com/mvitousek/reticulated/issues/43}}
 from the paper.
Second, Reticulated's type checker assigns the dynamic type to local
 variables in the "fully typed" benchmarks---often because the type system
 cannot articulate a more precise type.
Code that ends up with the dynamic type has fewer constraints to check at run-time.
Third, \citet{vss-popl-2017} use relatively small benchmarks.
Four have since been retired from the official Python benchmark suite
 because they are too small, unrealistic, and unstable.\footnote{Release notes: \url{https://pyperformance.readthedocs.io/changelog.html}}
On larger programs, Reticulated suffers from high overhead.
For example, a Reticulated version of our \texttt{sieve} benchmark runs in
 ~40 seconds without blame, and with blame times out after 10 minutes.

%% NOTE: Vitousek's benchmarks are from the Python "pyperformance" suite.
%%   The version notes in the docs talk about retiring benchmarks, but
%%   you can also look at the current codebase and see what names from POPL'17
%%   are missing: callsimple, callmethod, callmethodslots, & pystone
%% <https://github.com/python/pyperformance/tree/master/pyperformance/benchmarks>



