%% -----------------------------------------------------------------------------
\section{What We Learned}
\label{sec:discussion}

%% \begin{itemize}
%% \item Why does the data look like this?
%%   - why tr fails
%%     - bad runtime errors
%%     - TR bug
%%   - why Transient fails
%%     - bad runtime errors
%%     - timeouts/ooms: give context wrt fully untyped/typed versions
%%     - empty blame: trim Ben's prose

%%   \item Any kind of dynamic checking has a large positive impact, whether blame or stacktraces are used.
%%     \begin{itemize}
%%     \item Both in terms of usefulness and programmer effort
%%     \item Erasure extremely often fails to provide any helpful information.
%%     \item That said, there are no long trails for Erasure.
%%     \end{itemize}

%%   \item Blame helps significantly over just using stacktraces for both Natural and Transient.
%%     \begin{itemize}
%%     \item Natural's blame improves over exceptions by nearly 20\%
%%     \item Transient's blame improves over exceptions half as much
%%     \end{itemize}

%%   \item Natural clearly offers the most utility, but Transient is not too far behind.
%%     \begin{itemize}
%%     \item Natural clearly improves over all other modes
%%     \item Transient improves significantly over all other modes except Natural.
%%     \item Keep in mind that our selection of scenarios is biased toward Erasure & Transient.
%%     \end{itemize}
    

%%   \item Discussion of how confidence / margin of error should be interpreted wrt these results?


%%    \item Threats to validity:
%%         \begin{itemize}
%%           \item Racket stack traces
%%           \item Mutant sampling
%%           \item Blame trails do not necessarily capture all aspects of error reporting
%%           \item Transient blame adaptations
%%           \item More?

%%    \end{itemize}
%% \end{itemize}        

\subsection{Why does the data look like this?}

The figures in the last section raise a key question: why does blame fail in our dataset?
We analyze the reasons for failure separately for each mode of the rational programmer.

Breaking down the reasons for failure for all scenarios in which Natural is unsuccessful reveals two causes.
First, most of the failing scenarios begin with a runtime exception, which the rational programmer can use to type components until blame becomes available.
In these failing cases (there are 1708), however, blame never becomes available; the exception information points to the wrong component and the trail ends in failure.
A second, much smaller set (40 scenarios) do produce a runtime type error blaming some component, but the blame identifies a non-buggy typed component.
After careful inspection of these scenarios, we are confident that they reveal a bug in the implementation of Typed Racket.

Transient's blame failures break down in a similar way.
Like Natural, most failures occur with unhelpful exception information without ever getting blame (1851 scenarios).
However, Transient also has a substantial number of failures that occur because the programs hit the resource limits (timed out or ran out of memory) and had to be killed (some 770 scenarios).
Additionally, there are nearly a thousand cases where Transient's checks catch a problem, but the blame set reported is empty.
We discuss each of these last two failure categories in detail in~\ref{sec:threat:transient}.


\subsection{High level conclusions}

The analysis in the last section indicates a number of interesting conclusions about blame in the gradual typing setting.
First, figure~\ref{fig:avo-matrix} makes it clear that any kind of dynamic checking has a large positive impact over Erasure, and this is true regardless of whether the rational programmer uses blame or stacktraces.
Indeed, the bottom row of figure~\ref{fig:avo-matrix} illustrates that Erasure almost never offers more useful information than any other mode.
Nonetheless, figure~\ref{fig:effort-table} does show that even if Erasure often fails to provide helpful information, it provides the rational programmer with a quick answer.

Figure~\ref{fig:avo-matrix} also indicates that blame helps significantly over just using exception information.
Both Natural and Transient's blame modes improve over their corresponding exception modes by about ten percent.
This illustrates that blame tracking offers value for locating the source of mistakes beyond the value provided by the dynamic checks alone.
At only ten percent, however, the amount of added value suggests that foregoing blame tracking in order to obtain performance gains may not hurt debugging all too much.

Finally, the results show that while Natural clearly offers the most utility compared to the other modes, Transient is not too far behind.
Natural is more useful than every other mode in figure~\ref{fig:avo-matrix}, and by a larger proportion than any other mode.
That said, the same can be said of Transient's two blame modes as compared to every other mode but Natural.
Furthermore, the difference in usefulness between Natural and Transient's blame modes is quite small across the board.
As we point out in~\ref{sec:mutate}, however, we should note that our selection of scenarios for analysis introduces bias toward Erasure and Transient.


\subsection{Threats to validity}

Like {\em homo economicus\/}, which decouples the actual behavior of a
participant in an economy for the sake of mathematical modeling, the model of a
rational programmer decouples the actual debugging behavior of a software
developer for the sake of a systematic, large-scale analysis. This decoupling
comes with advantages and disadvantages. In the economic realm, mathematical models
have provided some predictive insights into the market's behavior; but as
behavioral economics has shown more recently, the mathematical abstraction of a
rational actor makes predictions also quite unreliable in some situations.
\footnote{It has also misled economists to focus on just the mathematics, though
this problem is not relevant here.}  Just like an ordinary consumer or producer,
an actual software developer is unlikely to stick to the exact strategy proposed
here. When this happens, the predicted benefits of blame assignment may not
materialize. Indeed, our own personal experience suggests such deviations, and
it also suggests that deviating is a mistake. To make a true judgment of the
usefulness of the rational-programmer idea, the community will need a lot more
experience with this form of evaluation and relating the evaluation to the
behavior of working programmers.

Relatedly, our setup hides the type choice a rational programmer must make. When the
run-time checks signal an impedance mismatch in the real world, a programmer
does not have a typed component ready to swap in. Instead, the programmer must
come up with the next set of types---and this means the programmer must make
choices. It is normally possible to assign consistent types to variables in a
component in different ways. The creation and curation of the benchmarks over
many years has driven home this lesson, but fortunately, it has also shown that
the types are in most cases reasonably canonical.  We therefore conjecture that
a rational programmer would in most cases come up with an equivalent type
assignment for trail extension as our experimental setup describes.


\subsection{Threat: Is our Transient Correct?}
\label{sec:threat:transient}
%% purpose: talk about transient-blame implementation .... what did we do,
%%  how did we test, what threats remain?
%%
%% the benchmark timeouts look like a threat, but this section shows that
%%  we've looked carefully at those

Surprisingly, Transient blames an empty set of boundaries in 967 cases.
Such cases should never occur, at least in theory, because an empty set
 means the value has never crossed a boundary---if the value is indeed defined
 in the current module, then we have typed code blaming itself for a typed
 value; thus, the type checker must be unsound.
After careful investigation of these empty blame cases, we found no soundness
 bugs in Typed Racket.
Instead, we found scenarios in which Transient lost track of the proper
 boundaries.
These scenarios suggest ways to improve the blame algorithm from~\citet{vss-popl-2017}:
\begin{itemize}
  \item
    Entries in a blame map must point to several parent entries.
    For example, if the function \texttt{f} receives bad input in the call
    \texttt{(filter f xs)}, then blame should point to both \texttt{filter}
    and the \texttt{xs} list.
  \item
    The construction of blame map entries must be guided by type-like specifications
     instead of relying on syntax.
    Aliasing the built-in \texttt{filter} function should not change the shape
     of the blame map.
  \item
    The initial blame map must reflect the initial type environment.
    For instance, Typed Racket trusts that untyped core-library functions, such as \texttt{filter},
     behave correctly; such assumptions should appear in the blame map.
\end{itemize}
\noindent{}Despite these known limitations, we conjecture that these improvements
 to Transient blame-tracking will not affect our overall conclusions.

A second apparent flaw in our Transient semantics is the high cost of blame.
Whereas \citet{vss-popl-2017} report an average slowdown of 2.5x and
worst-case of 5.4x on fully-typed benchmarks in Reticulated,
some of our configurations exceed a 4 minute timeout or 6GB memory limit with Transient blame.
To put those limits into context, the fully typed and fully untyped benchmarks all complete in a few seconds with minimal memory usage, and none of the mixed Natural configurations hit these limits.
Unfortunately, our high cost is closer to the truth;
 there are at least three broad issues that skew the earlier results.
First, the 2017 implementation of Reticulated fails to insert certain
 soundness checks\footnote{Missing check: \url{https://github.com/mvitousek/reticulated/issues/36}}
 and blame-map updates\footnote{Missing cast: \url{https://github.com/mvitousek/reticulated/issues/43}}
 from the paper.
Second, Reticulated's type checker assigns the dynamic type to local
 variables in the "fully typed" benchmarks---often because the type system
 cannot articulate a more precise type.
Code that ends up with the dynamic type has fewer constraints to check at run-time.
Third, \citet{vss-popl-2017} use relatively small benchmarks.
Four have since been retired from the official Python benchmark suite
 because they are too small, unrealistic, and unstable.\footnote{Release notes: \url{https://pyperformance.readthedocs.io/changelog.html}}
On larger programs, Reticulated suffers from high overhead.
For example, a Reticulated version of our \texttt{sieve} benchmark runs in
 ~40 seconds without blame, and with blame times out after 10 minutes.

%% NOTE: Vitousek's benchmarks are from the Python "pyperformance" suite.
%%   The version notes in the docs talk about retiring benchmarks, but
%%   you can also look at the current codebase and see what names from POPL'17
%%   are missing: callsimple, callmethod, callmethodslots, & pystone
%% <https://github.com/python/pyperformance/tree/master/pyperformance/benchmarks>



