%% -----------------------------------------------------------------------------

Implementing the method of the preceding section poses three challenges.  The
first concerns the comparison of the effect of blame on the rational programmer
across three different mechanisms; the second challenge is about finding a large
number of representative debugging scenarios; and the third is the resulting
huge space of possibilities. A coincidental challenge is the disparity of the
implementations of gradually typed languages. To eliminate this variable, the
authors use Racket, which is thus far the only language in which all three major
semantic variants are available in a robust and comparable manner: Typed Racket
implements Natural, Shallow Racket~\citep{ttt21} Transient, and plain Racket
Erasure.

The {\em first challenge\/} stems from the differences between the blame
assignment mechanisms of the three semantic variants.  While Natural assigns
blame to {\em one\/} component, Transient assigns blame to a sequence of
components. The Erasure semantics does not blame components {\it per se\/}, but
it comes with an exception location and a stack trace, which implicitly suggest
fixes.  Each strategy triggers different reactions by the rational programmer
(and real ones, too).

One way to reconcile these differences is to {\em equip the rational
programmer with modes \/} that represent the different types of
information the rational programmer takes into account when debugging a
scenario. Intuitively, different
blame strategies correspond to different modes of operation.
For instance, one Transient mode may assign types to the oldest element of a
blame sequence because it corresponds to the
earliest point in the execution that can discover an impedance
mismatch.  Another mode may opt to treat the sequence as a stack and add
types to its newest element.  If both modes are equally successful in
locating an impedance mismatch, measuring the rational programmer's effort with each mode
may answer which is the most effective.

However, attributing the success of the rational programmer to this or
that blame mechanism demands a careful analysis of the interplay between blame
and the run-time checks of each gradual type system. When a check fails,
the Natural and Transient semantics assign blame instead of using the
information in the exception from the run-time check. But, the
exception information may be as useful to the rational
programmer as a blame assignment. If this is the case, then blame {\em per se\/} may not play a critical
role for the rational programmer. Indeed, precisely because they do not
account for such confounding factors, \citet{lksfd-popl-2020} cannot draw any
conclusions about blame specifically, despite advertisements to the
opposite. Their experiment may conclude only that so-called blame-shifting
works, but they cannot attribute this conclusion to blame alone. 

Modes offer a uniform way to compare the different semantics and isolate blame
from the effect of the semantics' run-time checks. Specifically, the rational
programmer comes with a blame mode and an exception mode for Natural and
Transient. If the blame mode succeeds in debugging a program while the exception
one fails, it is safe to conclude that blame is indeed beneficial for the
rational programmer. Put differently, the exception mode serves as the {\em
baseline\/} for blame's value within a given semantics; if the programmer in
this mode performs as well or better than the blame one, a blame assignment
mechanism might be useless.

An experiment must also rule out that the usefulness of blame assignment is sheer
luck.  Hence, a completely random mode provides yet another necessary baseline.

The {\em second challenge\/} is to find a representative collection of programs
with impedance mismatches.\footnote{\citet{cc-oopsla-20} created a collection of
Reticulated Python programs to evaluate their technique of fixing mistakes in
type annotations. Their collection does not come with type-level mistakes in the
code itself.}  The impedance mismatches must represent mistakes that programmers accidentally
create and that the run-time checks of academic systems catch. In other words,
the experiment calls for a collection of mistakes in mixed-typed programs that is
representative of those ``in the wild.''  Unfortunately no such collection
exists, and with good reason. The kind of mistakes needed are typically detected
by unit or integration tests; even if it takes some time to find their sources,
these mistakes do not make it into code repositories with appropriate commit
messages.

An alternative is to {\em generate a corpus of 
mistakes \/} using mutation analysis~\citep{lipton1971fault, demillo1978hints,
jia2011analysis}, but conventional mutation analysis is useless.  Mutation
analysis traditionally aims to inject bugs that challenge test suites, and it
discards those that yield ill-typed mutants as \emph{incompetent}. Indeed,
mutation analysis frameworks are fine-tuned to avoid them, and yet, it is
precisely those mutators that are needed for evaluating blame assignment strategies.

Based on a related experience, \citet{gw-mutation} write,
``existing mutation frameworks \ldots\ do not generate the kinds of mutations
needed to best evaluate type annotations'' and, worse, ``it is surprisingly
difficult to come up with mutants that actually describe subtle type faults.''
While the goal of their work---to evaluate the quality of types in
Python---is unrelated to blame, the mechanism is related. And their
judgment confirms the experience of the authors. 

An experimental analysis of blame needs a mostly new set of mutators.
Roughly speaking, the new mutators inject type errors into fully typed programs.
Applying such a mutator to any typed component produces a mutated component.  A
debugging scenario results from removing the types from the mutated
component. For the design of such mutators, the authors relied on their own
extensive programming experience though not without discovering a major pitfall:
some of their original mutators systematically produced programs that
immediately revealed the source of the impedance mismatch.  All of the remaining
ones yield {\em interesting debugging scenarios\/} (see
sec.~\ref{sub:mutate-interesting}).

The {\em third challenge\/} is the explosive number of debugging scenarios that
result from the combination of mutation-based scenario generation and mode-based
analysis. All three factors---three different gradual typing systems, the large
number of mutants, and the number of debugging modes---contribute possibly
useful experimental data in a multiplicative manner. Hence, carried out naively,
the experiment would demand an infeasible amount of computational
resources.  A practical execution has no option but to {\em sample the space of
scenarios\/}, carefully ensuring reproducibility.

The next three sections explain how to overcome the three challenges
in detail. 
