%% -----------------------------------------------------------------------------

The rational programmer works like an algorithm. Implementing it demands a large
number of representative debugging scenarios; a reconciliation of three
different kinds of error messages; and a proper sampling strategy for the huge
space of possibilities. A coincidental challenge is the disparity of gradually
typed languages. To eliminate this variable from the comparison, this paper uses
Racket, which is thus far the only language in which all three major soundness
mechanisms are available in a reasonably robust and directly comparable manner;
for the implementation of the transient semantics, see the parallel submission
of an experience report~\cite{ttt21}.

The {\em first challenge\/} concerns the differences among the blame assignment
of the three semantic variants.  While Natural assigns blame to {\em one\/}
component, Transient assigns blame to a sequence of components. The Erasure
semantics does not blame a component {\it per se\/}, but it comes with an
exception location and a stack trace, which implicitly suggests parties to
blame.  The different strategies greatly affect the behavior of the rational
programmer (and real programmers, too).

One way to overcome this challenge is to {\em equip the rational programmer with
modes \/} that represent the alternative interpretations of blame.  For
instance, one Transient mode may pick the oldest element of a blame set as
the next component to add types because it corresponds to the earliest point in
the program's evaluation that can discover an impedance mismatch.  Another mode
may opt to treat the sequence as a stack and add types to its newest element.
When both modes are equally successful in locating an impedance mismatch,
measuring the rational programmer's effort may answer which mode is the most
effective.

Different blame information is also due to differences in the run-time checks
that the three gradual typing systems perform. This observation suggests another
mode for the rational programmer, namely, a mode that ignores the blame
information completely and instead inspects only the information in the
exceptions of the safety checks in the underlying languages.  This exception
mode serves as the {\em baseline\/} for blame's value; if the programmer in this
mode ``beats'' the others, blame systems might be useless.


The {\em second challenge\/} is to find a representative collection of
impedance mismatches. They must represent mistakes that programmers
accidentally create and that the run-time checks of academic systems catch. In
other words, the experiment calls for a collection of type mistakes in
mix-typed programs that is representative of those ``in the wild.''
Unfortunately no such collection exists, and with good reason. The kind of
mistakes needed are typically detected by unit or integration tests; even if it
takes some time to find their sources, these mistakes do not make it into code
repositories with appropriate commit messages.

An alternative is to {\em generate a large and representative corpus of type
mistakes \/} using mutation analysis~\cite{lipton1971fault, demillo1978hints,
jia2011analysis}. Conventional mutation analysis is useless, however.  Mutation
analysis traditionally aims to inject bugs that challenge test suites, and it
discards those that yield ill-typed mutants as \emph{incompetent}. Indeed,
mutation analysis frameworks are fine-tuned to avoid them, and yet, it is
precisely those that are needed for evaluating blame assignment strategies.

Based on a related experience, Gopinath and Walkingshaw~\cite{gw-mutation} write,
``existing mutation frameworks \ldots\ do not generate the kinds of mutations
needed to best evaluate type annotations'' and, worse, ``it is surprisingly
difficult to come up with mutants that actually describe subtle type faults.''
While the goal of their work---to evaluate the quality of types in
Python---is unrelated to the one here, the mechanism is related. And their
judgment confirms the experience of the authors. 

Thus, an experimental analysis of blame needs a mostly new set of mutators.
Roughly speaking, the new mutators inject type errors into fully typed programs.
Applying such a mutator to any typed component produces a mutatnt component.  A
debugging scenario results from removing the types from the mutatnt. For the
design of such mutators, the authors relied on their own extensive programming
experience though not without discovering a major pitfall: some of their
original mutators systematically produced programs that immediately revealed the
source of the impedance mismatch. About two thirds of the original mutators
survived this first scrutiny testing. All of the remaining ones yield {\em
interesting debugging scenarios\/} (see section~\ref{sub:mutate-interesting}).

The {\em third challenge\/} is the explosive number of debugging scenarios that
result from the combination of mutation-based scenario generation and mode-based
analysis. All three factors---three different gradual typing systems, the large
number of mutants, and the number of debugging modes---contribute possibly
useful experimental data in a multiplicative manner. Hence, carried out naively,
the experiment would demand an unacceptably large amount of computational
resources.  A practical execution has no option but to {\em sample the space of
scenarios\/}, carefully ensuring that it still produces reproducible
conclusions.

The next three sections explain how to overcome the three challenges
in detail. 


% Overcoming these challenges yields a systematic approach: 
% \begin{itemize}

% \item interesting mutators generate interesting type-level errors and thus
% debugging scenarios (section~\ref{sec:mutate});
    
% \item an algorithm simulates the modes of the rational programmer on these
% scenarios (section~\ref{sec:rational}); and

% \item a careful sampling of the space yields experimental
% data about the bug finding process (section~\ref{sec:sample}).
% \end{itemize}
% Getting all three aspects right is necessary to obtain a computationally
% feasible and statistically sound experiment.

% {\bf Note} A coincidental challenge is the disparity of gradually typed
% languages. While the three approaches to blame are well understood in theory,
% they have been implemented for rather different untyped languages and with
% vastly different type systems. To eliminate the effect of these differences, it
% is necessary to implement all three approaches in a single language so that an
% application of the evaluation method can use the same test programs, the same
% type system, and hence the same debugging scenarios.

% The results in this paper are for a system that builds on Greenman and
% Felleisen's effort of comparing the performance of Natural and Transient in
% Racket~\cite{gf-icfp-2018}. The extended system uses the type system of Typed
% Racket and comes with a Transient semantics that faithfully implements the
% design and the blame assignment strategy proposed by~\cite{vss-popl-2017}. The
% Erasure semantics drops the type information and runs the programs as if they
% were plain Racket programs. The next section explains this system in detail.
