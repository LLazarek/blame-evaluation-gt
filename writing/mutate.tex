\section{How to Make Lots of Mistakes} \label{sec:mutate}

An important challenge from section~\ref{sec:challenges} is the generation of
mix-typed programs with representative impedance mismatches between
the typed components, their types, and the behavior of untyped components. In this section, we
present a generation process and argue that, despite its synthetic
nature, it yields a large corpus of programs with  realistic and diverse type mistakes.

The starting point for our corpus of programs
is~\citet{gtnffvf-jfp-2019}'s gradual typing benchmark suite. The
benchmark suite consists of fully typed correct programs that are written by different authors
and have been used, maintained and evolved by their authors and others over a
number of years.  They range widely in size, complexity, purpose and
features of Typed Racket they employ.  Furthermore the benchmarks use advanced aspects
of Typed Racket's type system such as occurrence
typing~\cite{tf-icfp-2010}, types for mutable and immutable data
structures, polymorphic types, types for first-class classes and objects and types for
Racket's numeric tower~\cite{stathff-padl-12}. Without loss of the diversity of the benchmarks,
we select the ten largest in terms of numbers of components (between 6 and 14 components).
These benchmarks also have the most complex dependency graphs and, hence,
can make the debugging process the hardest for the rational programmer.
The table in figure~\ref{table:benchmark-descriptions} shows our selection
together with a short description for each benchmark.


\begin{figure}
\begin{tabular}{p{2cm} | p{10cm} }
  {\bf  name} & {\bf description (author)}  \\

\hline

  \texttt{acquire} & Board game simulation (M. Felleisen)  \\%[1em]


\hline
  \texttt{gregor} & Utilities for calendar dates (J. Zeppieri) \\%[1em]


\hline
  \texttt{kcfa} & Functional implementation of 2CFA for the lambda calculus (M. Might) \\%[1em]


\hline
  \texttt{quadT} & Converter from S-expresion source code to PDF format (M. Butterick)\\%[1em]

\hline
  \texttt{quadU} & Converter from S-expresion source code to PDF format  (B. Greenman) \\%[1em]

\hline
  \texttt{snake} & Functional implementation of the  Snake video game (D. Van Horn) \\%[1em]

\hline
  \texttt{suffixtree} & Algorithm for common longest subsequences between strings. (D. Yoo) \\%[1em]

\hline
  \texttt{synth} & Converter of description of notes and drum beats to WAV format (V. St-Amour \& N. Toronto) \\%[1em]

\hline
  \texttt{take5} & Card game simulator (M.Felleisen)  \\%[1em]

\hline
  \texttt{tetris} & Functional implementation of Tetris (D. Van Horn) \\%[1em]


\end{tabular}
  \caption{Benchmarks summary.}
  \label{table:benchmark-descriptions}
\end{figure}

Of course, the gradual typing benchmarks have no mistakes for the rational
programmer to debug. Therefore, we follow
~\citet{lksfd-popl-2020} and we inject bugs with mutation analysis.
A mutation is a local syntactic change of the code of a program that may
produce a bug.
\ll{Need citations for mutation}
Usually, the outcome of mutating a program
once, dubbed a mutant, can be distinguished from the original program with a test case.
The better the test suite of a program, the more mutants it can ``kill.''
Thus a big part of research on mutation analysis goes into designing
operations that perform mutations, dubbed mutators, that create mutants
that are hard to ``kill.'' Unlike the standard mutators, which \citet{lksfd-popl-2020}
use, too, the mutators we need to evaluate the rational programmer have to be
fine-tuned to generate shallow errors. Typed Racket's type system detects
typos not logical mistakes, and mutators suitable for our purposes
should target the former rather than the latter.

The table in figure~\ref{table:mutation-ops} summarizes our mutators.
For each one, the table provides a short description and an example
mutation it performs. Each mutator is inspired by the more-than-a-decade
long experience of the authors making type-level mistakes in Typed Racket.
We find that these mistakes often take non-trivial effort to debug.

Most of our mutators in figure~\ref{table:mutation-ops} are self-explanatory.
The first twelve can apply to most gradually typed languages, including those with classes.
The last four mutators target distinguishing features of Typed Racket.
Specifically, \texttt{arithmetic} may replace a \texttt{+} with a \texttt{-}
in an attempt to change the type of the result of the arithmetic
operation. In Typed Racket, \texttt{+}'s result is a
\texttt{Positive-Integer} when all arguments are
\texttt{Positive-Integer}. However, the result of \texttt{-} is
\texttt{Integer}. In the same spirit, \texttt{boolean} aims to take
advantage of Typed Racket's ``truthiness.'' Finally, \texttt{negate-cond} and \texttt{force-cond}
aim to confuse occurrence typing.


\begin{figure}
  \begin{tabular}{p{2.1cm} | p{5cm}  | p{5.5cm} }
    {\bf name} & {\bf description} & {\bf example} \\




    \texttt{constant}
& Swap a literal constant with another of the same value but different type.
& \texttt{5.6} $\rightarrow$ \texttt{5.6+0.0i} \\
\hline

    \texttt{deletion}
    & Deletes the result expression of a sequence.
    & \texttt{(begin x y z)} $\rightarrow$ \texttt{(begin x y)} \\
\hline

    \texttt{top-level-id}
 & Swaps two identifiers defined in the same module.
 &\texttt{(f x 42)} $\rightarrow$ \texttt{(g x 42)}\\
\hline

    \texttt{imported-id}
 & Swaps two identifiers imported from the same module.
 &\texttt{(f x 42)} $\rightarrow$ \texttt{(g x 42)}\\
\hline

    \texttt{method-id}
 & Swaps two method identifiers.
 &\texttt{(send o f x 42)} $\rightarrow$ \texttt{(send o g x 42)}\\
\hline

    \texttt{field-id}
 & Swaps two field identifiers.
    & \begin{tabular}[t]{@{}l} \texttt{(get-field o f)} $\rightarrow$ \\
        \texttt{(get-field o g)}
      \end{tabular}\\
\hline

   \texttt{position}
 & Swaps two subexpressions.
  &  \texttt{(f a 42 "b" 0)} $\rightarrow$\texttt{(f a 42 0 "b")} \\
\hline

     \texttt{list}
    & Replaces \texttt{append} with \texttt{cons}.
    &  \begin{tabular}[t]{@{}l}
      \texttt{append} $\rightarrow$ \texttt{cons}
    \end{tabular}\\
\hline

     \texttt{class:init}
  & Swaps values of class initializers.
   &  \begin{tabular}[t]{@{}l}
     \texttt{(new c [a 5] [b "hello"])} $\rightarrow$\\
     \texttt{(new c [a "hello"] [b 5])}\\
    \end{tabular}\\
\hline

     \texttt{class:parent}
    & Replaces the parent of classes with \texttt{object\%}.
    &  \begin{tabular}[t]{@{}l}
     \texttt{(class foo\% (super-new))} $\rightarrow$\\
     \texttt{(class object\% (super-new))}\\
    \end{tabular}\\
\hline

     \texttt{class:public}
    & Makes a public method private and vice versa.
    & \begin{tabular}[t]{@{}l}
      \texttt{(class object\%}\\
      \phantom{(d}\texttt{(define/public (m x) x))} $\rightarrow$\\
      \texttt{(class object\%}\\
      \phantom{(d}\texttt{(define/private (m x) x))}\\
    \end{tabular}\\
\hline

     \texttt{class:super}
    & Removes \texttt{super-new} calls from class definitions.
    &  \begin{tabular}[t]{@{}l}
      \texttt{(class foo\% (super-new))} $\rightarrow$\\
      \texttt{(class foo\% (void))}\\
    \end{tabular}\\
\hline


     \texttt{arithmetic}
& Swaps arithmetic operators.
    & \texttt{+} $\rightarrow$ \texttt{-} \\
\hline

     \texttt{boolean}
& Swaps \texttt{and} and \texttt{or}.
& \texttt{and} $\rightarrow$ \texttt{or} \\
\hline

     \texttt{negate-cond}
    & Negates conditional test expressions.
    &  \begin{tabular}[t]{@{}l}
      \texttt{(if (= x 0) t e)} $\rightarrow$\\
      \texttt{(if (not (= x 0)) t e)}
       \end{tabular}\\
\hline

     \texttt{force-cond}
    & Replaces conditional test expressions with \texttt{\#t}.
    &  \begin{tabular}[t]{@{}l}
      \texttt{(if (= x 0) t e)} $\rightarrow$\\
      \texttt{(if \#t t e)}
    \end{tabular}\\
\hline
\end{tabular}
  \caption{Summary of mutators.}
  \label{table:mutation-ops}
\end{figure}


\subsection{Do these mutators give us interesting debugging scenarios?}
\begin{itemize}
\item do these mutators work?
\begin{itemize}
\item now we need to see if these mutators actually produce interesting errors
\begin{itemize}
\item define interesting errors: type error + dynamic err w/ 3 modules on stack in a mixed configuration
\item this section will show if they do this on our benchmarks
\end{itemize}
\item do they actually produce interesting errors? -- for this need plot that's inverted version dyn-err-heatmap + total of mutants per benchmark
\item do we need all of the mutators?
\begin{itemize}
\item each one produces type errors + interesting scenarios
\item we conjecture they represent different kinds of mistakes
\end{itemize}
\end{itemize}
\end{itemize}

Having selected a set of mutators based on our experience, we next need to see if the mutators do in fact produce interesting debugging scenarios in our programs.
We define an interesting debugging scenario as a mutation that
\begin{enumerate}
\item can be caught by the type checker if the program is fully typed,
\item otherwise produces a dynamic error under Erasure, and
\item the stacktrace of the dynamic error contains at least three distinct modules from the program.
\end{enumerate}
The first criterion reflects the fact that a gradual type system can only catch bugs that would have been caught by a type checker.
The second criterion ensures that the bug is detectable by all three semantics.
It is based on~\citet{gfd-oopsla-2019}'s proof that a dynamic type error in Erasure implies a dynamic type error in both Transient and Natural.
The final criterion conservatively approximates scenarios with the opportunity for interactions between typed and untyped modules. % I say opportunity because it depends on the config
In particular, it takes at least two modules to have typed/untyped module interactions (one of each kind), and having two modules on the stack ensures that the error arose in the interaction of at least those two modules.
The third module comes in because every one of our programs comes with a driver module which is usually uninteresting but should be present in every stacktrace.
With this definition in hand, the rest of this section will analyze how well our mutators produce interesting debugging scenarios.

Beginning with the first criterion of the definition, we analyze how well our mutators produce type errors.



%% Cut from above
An effective set of mutators for type errors should be able to generate
a large number of ill-typed programs. Hence, to validate that our experience
with Typed Racket translates to effective mutators, we analyze the
 mutants of our ten benchmarks. Figure~\ref{fig:mutators-static-hit} shows
 the results. For each benchmark, our mutators generate between 156 and
 18165 ill-typed mutants. Most importantly the collective success rate of
 the mutators  per benchmark, i.e., the percentage of ill-typed mutants
 over the number of mutants, ranges between 53\% (\texttt{tetris})
 and 95\% (\texttt{kCFA}). Finally, the figure depicts the individual success rate
 of mutators per benchmark which reveals that there is at
 least one benchmark where each mutator performs well
 (>90\%).

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./plots/code-hit-ratios}
  \caption{Success rate of mutators (the percentage of ill-typed mutants
  over the number of mutants)}
  \label{fig:mutators-static-hit}
\end{figure}



%% ll: Is this still necessary?
As a
final remark in this section, with the data from
figure~\ref{fig:mutators-runtime-hit}, we verify  that in each of the three semantics
we target, between 25\% and
90\% of the ill-typed mutants each mutator generates give rise to at least one interesting debugging scenario.


\subsection{What are interesting debugging scenarios?}
So far we have established that our mutators and~\citet{gtnffvf-jfp-2019}'s
benchmarks are sufficient to create a large number of realistic ill-typed Typed Racket
programs. However, in an ill-typed program with correct types, like
in our case, it is trivial to figure out which component contains the bug.
It is the component that doesn't type check. Thus the ill-typed mutants
do not offer an interesting setting to try out the debugging abilities of
rational programmers. That said, we can derive interesting debugging
scenarios by removing type specifications from some of the components of
the benchmarks. The resulting mix-typed programs may be well-typed and when
run may raise a type error. Section~\ref{sec:rational} explains how we can
explore the space of debugging scenarios in a principled manner starting from
an interesting scenario.
In the rest of this section we describe how we select interesting scenarios.
We break this decision into three basic factors, each of which specify an element defining our selection of interesting scenarios.

First, not all debugging scenarios are suitable for evaluating blame across all three semantics.
For instance, there exist scenarios where Natural's checks discover a problem but Transient and Erasure do not.
Since Transient and Erasure provide no debugging information (and in particular no blame) in such cases, we have nothing to evaluate.
As alluded to in the previous paragraph, scenarios where the type checker detects the problem statically are also uninteresting.
Hence, we select only scenarios which are interesting in all three semantics.
Based on~\citet{gfd-oopsla-2019}'s proof that a dynamic type error in erasure implies a dynamic type error in both Transient and Natural, for any scenario that Erasure detects the problem, both Transient and Natural should detect it as well.
We can therefore define scenarios that raise a dynamic type error in Erasure as interesting, and all such scenarios should raise dynamic type errors in Transient and Natural, too.

The second factor is that we would like to be able to compare the behavior of blame between each of our three semantics.
In order to do so, we must evaluate each of the semantics on the same set of scenarios;
this allows us to compare for any given scenario how Natural compares to Erasure, for instance.
Hence, we pre-select a sample of scenarios which serve as the starting points to evaluate all three semantics.

Finally, the third factor is that we only want to consider debugging scenarios in which the bug has effects that cross component boundaries.
In the context of migratory typing, dynamic type enforcement occurs when values cross component boundaries.
However, a bug may be caught by the runtime before any values it influences flow out of the containing component -- for instance, if the bug is that a direct argument to \texttt{+} becomes a string.
We consider such scenarios trivial, becase typing the component in which the problem manifests immediately identifies that same component as the buggy one.
In an effort to avoid these trivial scenarios, we further restrict our definition of interesting debugging scenarios to be those in which the stack trace from Erasure identifies a component other than the buggy one.

Thus we have defined the set of interesting debugging scenarios as those for which Erasure raises a dynamic type error in some component other than the buggy one.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./plots/code-dynamic-error}
  \caption{}
  \label{fig:mutators-runtime-hit}
\end{figure}

